{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c63de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def load_prompts(path=\"prompts.json\"):\n",
    "    ret = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        prompts = json.load(f)  # 修正这里\n",
    "        for p in prompts:\n",
    "            ret[p[\"category\"]] = p\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efd6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_prompt(question, answer, question_type, reference, prompts):\n",
    "    # question_type: factual, multiple_choice, instruction, roleplay, open_ended\n",
    "    mapping = {\n",
    "        \"factual\": \"knowledge\",\n",
    "        \"multiple_choice\": \"multiple_choice\",\n",
    "        \"instruction\": \"instruction\",\n",
    "        \"roleplay\": \"roleplay\",\n",
    "        \"open_ended\": \"open_ended\",\n",
    "    }\n",
    "    prompt_key = mapping.get(question_type, \"knowledge\")\n",
    "    prompt = prompts[prompt_key]\n",
    "    sys_prompt = prompt[\"system_prompt\"]\n",
    "    if prompt_key == \"multiple_choice\":\n",
    "        prompt_text = prompt[\"prompt_template\"].format(\n",
    "            question=question, answer=answer, correct_answer=reference or \"\"\n",
    "        )\n",
    "    else:\n",
    "        prompt_text = prompt[\"prompt_template\"].format(question=question, answer=answer)\n",
    "    return sys_prompt, prompt_text\n",
    "\n",
    "\n",
    "def evaluate_answer(\n",
    "    model, question, answer, question_type, reference, prompts, eval_model=\"gpt-4o\"\n",
    "):\n",
    "    sys_prompt, eval_prompt = get_eval_prompt(\n",
    "        question, answer, question_type, reference, prompts\n",
    "    )\n",
    "    payload = json.dumps(\n",
    "        {\n",
    "            \"model\": eval_model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": eval_prompt},\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"sk-************************************************\",\n",
    "        \"User-Agent\": \"DeerAPI/1.0.0 (https://api.deerapi.com)\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }F\n",
    "    # return 10, \"good\"\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            \"https://api.deerapi.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            data=payload,\n",
    "            timeout=60,\n",
    "        )\n",
    "        content = resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        with open(f\"evaluations/{model}_eval_prompt.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"问题: \\n{question}\\n\")\n",
    "            f.write(f\"回答: \\n{answer}\\n\")\n",
    "            f.write(f\"评估提示: \\n{eval_prompt}\\n\")\n",
    "            f.write(f\"评估结果: \\n{content}\\n\\n\")\n",
    "\n",
    "        score_match = re.search(r\"\\[\\[(\\d+)\\]\\]\", content)\n",
    "        score = int(score_match.group(1)) if score_match else None\n",
    "        return score, content\n",
    "    except Exception as e:\n",
    "        return None, f\"评估失败: {e}\"\n",
    "\n",
    "\n",
    "def evaluate_model_result(model, prompts, eval_model=\"gpt-4.1\"):\n",
    "    os.makedirs(\"evaluations\", exist_ok=True)\n",
    "    with open(f\"result/{model}_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        results = [json.loads(line) for line in f if line.strip()]\n",
    "    eval_results = []\n",
    "    for idx, item in enumerate(results):\n",
    "        question = item.get(\"question\")\n",
    "        answer = item.get(\"response\")\n",
    "        question_type = item.get(\"question_type\", \"factual\")\n",
    "        reference = None\n",
    "        if \"reference\" in item:\n",
    "            reference = (\n",
    "                item[\"reference\"][0]\n",
    "                if isinstance(item[\"reference\"], list)\n",
    "                else item[\"reference\"]\n",
    "            )\n",
    "        score, eval_text = evaluate_answer(\n",
    "            model, question, answer, question_type, reference, prompts, eval_model\n",
    "        )\n",
    "        item[\"evaluation_score\"] = score\n",
    "        item[\"evaluation_text\"] = eval_text\n",
    "        eval_results.append(item)\n",
    "        print(f\"模型{model} 问题{item.get('question_id')} 评分: {score}\")\n",
    "        # time.sleep(1)  # 防止API限流\n",
    "    with open(f\"evaluations/{model}_evaluation.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in eval_results:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"{model} 评估完成，结果已保存到 evaluations/{model}_evaluation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd11c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有模型都有回答，可以开始评估\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"deepseek-chat\",\n",
    "    \"claude-3-5-haiku-20241022\",\n",
    "    \"gemini-2.5-flash-preview-04-17\",\n",
    "]\n",
    "\n",
    "nice = True\n",
    "for model in models:\n",
    "    with open(f\"result/{model}_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        results = [json.loads(line) for line in f]\n",
    "    for result in results:\n",
    "        question = result.get(\"question\")\n",
    "        answer = result.get(\"response\")\n",
    "        if answer is None:\n",
    "            nice = False\n",
    "            print(f\"模型{model} 问题{result.get('question_id')} 没有回答\")\n",
    "\n",
    "if nice:\n",
    "    print(\"所有模型都有回答，可以开始评估\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e5d9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'knowledge': {'name': 'knowledge-v1', 'type': 'single', 'system_prompt': 'You are an expert evaluator with deep domain knowledge.', 'prompt_template': \"[Instruction]\\nYou are evaluating the response to a factual knowledge question. Please assess the accuracy, completeness, and clarity of the answer. Consider whether all key facts are correctly presented, whether the answer addresses all aspects of the question, and whether the explanation is clear and well-structured. After your evaluation, rate the response on a scale of 1-10 by providing your rating in this format: [[rating]].\\n\\n[Question]\\n{question}\\n\\n[The Start of Assistant's Answer]\\n{answer}\\n[The End of Assistant's Answer]\", 'description': 'Prompt for evaluating factual knowledge questions', 'category': 'knowledge', 'output_format': '[[rating]]'}, 'multiple_choice': {'name': 'multiple-choice-v1', 'type': 'single', 'system_prompt': 'You are an objective evaluator for multiple-choice questions.', 'prompt_template': \"[Instruction]\\nPlease evaluate the assistant's response to this multiple-choice question. First, determine if the assistant selected the correct answer option. Then, assess the quality of explanation provided. A good response should clearly identify the correct option and provide accurate reasoning for why that option is correct and why others are incorrect. After your evaluation, rate the response on a scale of 1-10 by providing your rating in this format: [[rating]].\\n\\n[Question]\\n{question}\\n\\n[The Start of Assistant's Answer]\\n{answer}\\n[The End of Assistant's Answer]\\n\\n[The Correct Answer]\\n{correct_answer}\", 'description': 'Prompt for evaluating multiple-choice questions', 'category': 'multiple_choice', 'output_format': '[[rating]]'}, 'instruction': {'name': 'instruction-v1', 'type': 'single', 'system_prompt': 'You are an expert evaluator assessing how well instructions were followed.', 'prompt_template': \"[Instruction]\\nPlease evaluate how well the assistant followed the given instructions. Consider whether all requirements were addressed, the quality and correctness of the work produced, and whether the response demonstrates appropriate expertise in the subject matter. After your evaluation, rate the response on a scale of 1-10 by providing your rating in this format: [[rating]].\\n\\n[Question]\\n{question}\\n\\n[The Start of Assistant's Answer]\\n{answer}\\n[The End of Assistant's Answer]\", 'description': 'Prompt for evaluating instruction-following', 'category': 'instruction', 'output_format': '[[rating]]'}, 'roleplay': {'name': 'roleplay-v1', 'type': 'single', 'system_prompt': 'You are evaluating the quality of role-playing by an AI assistant.', 'prompt_template': \"[Instruction]\\nPlease evaluate how effectively the assistant adopted the requested role or perspective. Consider whether the response demonstrates appropriate domain knowledge for the assigned role, maintains the correct perspective throughout, and provides insights that would be expected from a genuine expert in that role. After your evaluation, rate the response on a scale of 1-10 by providing your rating in this format: [[rating]].\\n\\n[Question]\\n{question}\\n\\n[The Start of Assistant's Answer]\\n{answer}\\n[The End of Assistant's Answer]\", 'description': 'Prompt for evaluating role-playing responses', 'category': 'roleplay', 'output_format': '[[rating]]'}, 'open_ended': {'name': 'open-ended-v1', 'type': 'single', 'system_prompt': 'You are evaluating the quality of responses to complex open-ended questions.', 'prompt_template': \"[Instruction]\\nPlease evaluate the response to this open-ended question. Consider the depth of analysis, balance of perspectives, quality of reasoning, and creativity of insights. A good response should demonstrate nuanced understanding of the topic, consider multiple viewpoints, provide well-reasoned arguments, and offer original insights. After your evaluation, rate the response on a scale of 1-10 by providing your rating in this format: [[rating]].\\n\\n[Question]\\n{question}\\n\\n[The Start of Assistant's Answer]\\n{answer}\\n[The End of Assistant's Answer]\", 'description': 'Prompt for evaluating open-ended responses', 'category': 'open_ended', 'output_format': '[[rating]]'}}\n"
     ]
    }
   ],
   "source": [
    "prompts = load_prompts(\"prompts.json\")\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ff6d660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型gemini-2.5-flash-preview-04-17 问题101 评分: 9\n",
      "模型gpt-3.5-turbo 问题101 评分: 9\n",
      "模型deepseek-chat 问题101 评分: 8\n",
      "模型claude-3-5-haiku-20241022 问题101 评分: 9\n",
      "模型gpt-3.5-turbo 问题102 评分: 10\n",
      "模型deepseek-chat 问题102 评分: 10\n",
      "模型gemini-2.5-flash-preview-04-17 问题102 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题102 评分: 10\n",
      "模型deepseek-chat 问题103 评分: 10\n",
      "模型gpt-3.5-turbo 问题103 评分: 10\n",
      "模型gemini-2.5-flash-preview-04-17 问题103 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题103 评分: 9\n",
      "模型deepseek-chat 问题104 评分: 9\n",
      "模型deepseek-chat 问题105 评分: 9\n",
      "模型gpt-3.5-turbo 问题104 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题104 评分: 9\n",
      "模型gpt-3.5-turbo 问题105 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题105 评分: 10\n",
      "模型claude-3-5-haiku-20241022 问题201 评分: 10\n",
      "模型gemini-2.5-flash-preview-04-17 问题104 评分: 9\n",
      "模型gpt-3.5-turbo 问题201 评分: 10\n",
      "模型deepseek-chat 问题201 评分: 10\n",
      "模型gemini-2.5-flash-preview-04-17 问题105 评分: 10\n",
      "模型deepseek-chat 问题202 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题201 评分: 10\n",
      "模型gpt-3.5-turbo 问题202 评分: 10\n",
      "模型claude-3-5-haiku-20241022 问题202 评分: 8\n",
      "模型deepseek-chat 问题203 评分: 10\n",
      "模型claude-3-5-haiku-20241022 问题203 评分: 10\n",
      "模型deepseek-chat 问题204 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题202 评分: 9\n",
      "模型gpt-3.5-turbo 问题203 评分: 8\n",
      "模型claude-3-5-haiku-20241022 问题204 评分: 10\n",
      "模型gpt-3.5-turbo 问题204 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题203 评分: 10\n",
      "模型claude-3-5-haiku-20241022 问题205 评分: 10\n",
      "模型deepseek-chat 问题205 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题204 评分: 10\n",
      "模型gpt-3.5-turbo 问题205 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题301 评分: 6\n",
      "模型gemini-2.5-flash-preview-04-17 问题205 评分: 9\n",
      "模型deepseek-chat 问题301 评分: 6\n",
      "模型gpt-3.5-turbo 问题301 评分: 7\n",
      "模型gemini-2.5-flash-preview-04-17 问题301 评分: 7\n",
      "模型claude-3-5-haiku-20241022 问题302 评分: 9\n",
      "模型gpt-3.5-turbo 问题302 评分: 10\n",
      "模型gpt-3.5-turbo 问题303 评分: 10\n",
      "模型claude-3-5-haiku-20241022 问题303 评分: 10\n",
      "模型deepseek-chat 问题302 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题302 评分: 10\n",
      "模型deepseek-chat 问题303 评分: 10\n",
      "模型gpt-3.5-turbo 问题304 评分: 10\n",
      "模型gemini-2.5-flash-preview-04-17 问题303 评分: 10\n",
      "模型claude-3-5-haiku-20241022 问题304 评分: 10\n",
      "模型gpt-3.5-turbo 问题305 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题305 评分: 10\n",
      "模型gemini-2.5-flash-preview-04-17 问题304 评分: 10\n",
      "模型gemini-2.5-flash-preview-04-17 问题305 评分: 10\n",
      "模型gpt-3.5-turbo 问题401 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题401 评分: 9\n",
      "模型deepseek-chat 问题304 评分: 10\n",
      "模型claude-3-5-haiku-20241022 问题402 评分: 10\n",
      "模型deepseek-chat 问题305 评分: 9\n",
      "模型gpt-3.5-turbo 问题402 评分: 10\n",
      "模型claude-3-5-haiku-20241022 问题403 评分: 10\n",
      "模型gpt-3.5-turbo 问题403 评分: 10\n",
      "模型gemini-2.5-flash-preview-04-17 问题401 评分: 9\n",
      "模型deepseek-chat 问题401 评分: 10\n",
      "模型claude-3-5-haiku-20241022 问题404 评分: 9\n",
      "模型gpt-3.5-turbo 问题404 评分: 9\n",
      "模型gpt-3.5-turbo 问题405 评分: 10\n",
      "模型deepseek-chat 问题402 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题402 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题405 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题403 评分: 10\n",
      "模型deepseek-chat 问题403 评分: 9\n",
      "模型gpt-3.5-turbo 问题501 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题404 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题501 评分: 9\n",
      "模型gpt-3.5-turbo 问题502 评分: 7\n",
      "模型deepseek-chat 问题404 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题405 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题502 评分: 6\n",
      "模型deepseek-chat 问题405 评分: 9\n",
      "模型gpt-3.5-turbo 问题503 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题501 评分: 9\n",
      "模型deepseek-chat 问题501 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题502 评分: 7\n",
      "模型gpt-3.5-turbo 问题504 评分: 9\n",
      "模型claude-3-5-haiku-20241022 问题503 评分: 9\n",
      "模型gpt-3.5-turbo 问题505 评分: 8\n",
      "gpt-3.5-turbo 评估完成，结果已保存到 evaluations/gpt-3.5-turbo_evaluation.json\n",
      "模型deepseek-chat 问题502 评分: 3\n",
      "模型gemini-2.5-flash-preview-04-17 问题503 评分: 10\n",
      "模型deepseek-chat 问题503 评分: 9\n",
      "模型gemini-2.5-flash-preview-04-17 问题504 评分: 8\n",
      "模型claude-3-5-haiku-20241022 问题504 评分: 8\n",
      "模型gemini-2.5-flash-preview-04-17 问题505 评分: 10\n",
      "gemini-2.5-flash-preview-04-17 评估完成，结果已保存到 evaluations/gemini-2.5-flash-preview-04-17_evaluation.json\n",
      "模型claude-3-5-haiku-20241022 问题505 评分: 10\n",
      "claude-3-5-haiku-20241022 评估完成，结果已保存到 evaluations/claude-3-5-haiku-20241022_evaluation.json\n",
      "模型deepseek-chat 问题504 评分: 8\n",
      "模型deepseek-chat 问题505 评分: 10\n",
      "deepseek-chat 评估完成，结果已保存到 evaluations/deepseek-chat_evaluation.json\n"
     ]
    }
   ],
   "source": [
    "# 评估所有模型\n",
    "threads = [Thread(target=evaluate_model_result, args=(model, prompts)) for model in models]\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
